{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NER 데이타셋을 만들기 위한 handler\n",
    "190306\n",
    "pdf -> plaintext 가 주어지면\n",
    "\n",
    "1) 문장단위로 분절\n",
    "\n",
    "2) 문장단위 분절 정제\n",
    "                                                                              \n",
    "3) 문장을 단어단위로 분절\n",
    " \n",
    "수행"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1) 문장 단위 분절\n",
    "plaintxt 를 문장단위로 분절"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = '현재 TED 웹사이트에는 1,000개가 넘는 TED강연들이 있습니다. 여기 계신 여러분의 대다수는 정말 대단한 일이라고 생각하시겠죠 -- 전 다릅니다. 전 그렇게 생각하지 않아요. 저는 여기 한 가지 문제점이 있다고 생각합니다. 왜냐하면 강연이 1,000개라는 것은, 공유할 만한 아이디어들이 1,000개 이상이라는 뜻이 되기 때문이죠. 도대체 무슨 수로 1,000개나 되는 아이디어를 널리 알릴 건가요?'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/go/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "!pip install nltk==3.2.5\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "import re\n",
    "from nltk.tokenize import sent_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = '현재 TED 웹사이트에는 1,000개가 넘는 TED강연들이 있습니다. 여기 계신 여러분의 대다수는 정말 대단한 일이라고 생각하시겠죠 -- 전 다릅니다. 전 그렇게 생각하지 않아요. 저는 여기 한 가지 문제점이 있다고 생각합니다. 왜냐하면 강연이 1,000개라는 것은, 공유할 만한 아이디어들이 1,000개 이상이라는 뜻이 되기 때문이죠. 도대체 무슨 수로 1,000개나 되는 아이디어를 널리 알릴 건가요?'\n",
    "# if data.strip() != \"\":\n",
    "data = re.sub(r'([a-z])\\.([A-Z])', r'\\1. \\2', data.strip())\n",
    "data_sentences = sent_tokenize(data.strip())\n",
    "# sent_tokenize(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['현재 TED 웹사이트에는 1,000개가 넘는 TED강연들이 있습니다.',\n",
       " '여기 계신 여러분의 대다수는 정말 대단한 일이라고 생각하시겠죠 -- 전 다릅니다.',\n",
       " '전 그렇게 생각하지 않아요.',\n",
       " '저는 여기 한 가지 문제점이 있다고 생각합니다.',\n",
       " '왜냐하면 강연이 1,000개라는 것은, 공유할 만한 아이디어들이 1,000개 이상이라는 뜻이 되기 때문이죠.',\n",
       " '도대체 무슨 수로 1,000개나 되는 아이디어를 널리 알릴 건가요?']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_sentences"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2) 문장단위 분절 정제\n",
    "문장 합치기 및 분절 예제\n",
    "\n",
    "여러 라인에 한 문장이 들어 있는 경우에 대한 파이썬 스크립트 예제."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_data = ['현재 TED 웹사이트에는 1,000개가 넘는 TED강연들이 있습니다.',\n",
    "        '여기 계신 여러분의 대다수는',\n",
    "        '정말 대단한 일이라고 생각하시겠죠 --',\n",
    "        '전 다릅니다. 전 그렇게 생각하지 않아요.',\n",
    "        '저는 여기 한 가지 문제점이 있다고 생각합니다.',\n",
    "        '왜냐하면 강연이 1,000개라는 것은,',\n",
    "        '공유할 만한 아이디어들이 1,000개 이상이라는 뜻이 되기 때문이죠.',\n",
    "        '도대체 무슨 수로',\n",
    "        '1,000개나 되는 아이디어를 널리 알릴 건가요?',\n",
    "        '1,000개의 TED 영상 전부를 보면서']"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "## !! label\n",
    "현재 TED 웹사이트에는 1,000개가 넘는 TED강연들이 있습니다.\n",
    "여기 계신 여러분의 대다수는 정말 대단한 일이라고 생각하시겠죠 -- 전 다릅니다.\n",
    "전 그렇게 생각하지 않아요.\n",
    "저는 여기 한 가지 문제점이 있다고 생각합니다.\n",
    "왜냐하면 강연이 1,000개라는 것은, 공유할 만한 아이디어들이 1,000개 이상이라는 뜻이 되기 때문이죠.\n",
    "도대체 무슨 수로 1,000개나 되는 아이디어를 널리 알릴 건가요?\n",
    "1,000개의 TED 영상 전부를 보면서"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "Normalized_sentence_tokenized = []\n",
    "\n",
    "buf = []\n",
    "for line in raw_data:\n",
    "    if line.strip() != \"\":\n",
    "        buf += [line.strip()]  # 공백 제거한 문장들 buf에 쌓기\n",
    "        sentences = sent_tokenize(\" \".join(buf))  # 글자별로 한칸씩 더 띄어쓰기\n",
    "        \n",
    "        if len(sentences) > 1:\n",
    "            buf = sentences[1:]  # 바로 뒷문장 buf로 가져오기\n",
    "#             print(sentences[0] + '\\n')\n",
    "            Normalized_sentence_tokenized.append(sentences[0] + '\\n')\n",
    "# print(\" \".join(buf) + \"\\n\")\n",
    "Normalized_sentence_tokenized.append(\" \".join(buf) + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['현재 TED 웹사이트에는 1,000개가 넘는 TED강연들이 있습니다.\\n',\n",
       " '여기 계신 여러분의 대다수는 정말 대단한 일이라고 생각하시겠죠 -- 전 다릅니다.\\n',\n",
       " '전 그렇게 생각하지 않아요.\\n',\n",
       " '저는 여기 한 가지 문제점이 있다고 생각합니다.\\n',\n",
       " '왜냐하면 강연이 1,000개라는 것은, 공유할 만한 아이디어들이 1,000개 이상이라는 뜻이 되기 때문이죠.\\n',\n",
       " '도대체 무슨 수로 1,000개나 되는 아이디어를 널리 알릴 건가요?\\n',\n",
       " '1,000개의 TED 영상 전부를 보면서\\n']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Normalized_sentence_tokenized"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3) 문장을 단어단위로 분절\n",
    "문장을 단어 단위로 분절"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: nltk==3.2.5 in /home/go/anaconda3/envs/venv/lib/python3.6/site-packages (3.2.5)\n",
      "Requirement already satisfied: six in /home/go/anaconda3/envs/venv/lib/python3.6/site-packages (from nltk==3.2.5) (1.11.0)\n",
      "[nltk_data] Downloading package perluniprops to /home/go/nltk_data...\n",
      "[nltk_data]   Package perluniprops is already up-to-date!\n",
      "[nltk_data] Downloading package nonbreaking_prefixes to\n",
      "[nltk_data]     /home/go/nltk_data...\n",
      "[nltk_data]   Package nonbreaking_prefixes is already up-to-date!\n",
      "['i', 'love', 'myself', 'too', 'much', '.']\n",
      "['i', 'am', 'so', 'hungry']\n",
      "['where', 'are', 'you', 'going']\n"
     ]
    }
   ],
   "source": [
    "!pip install nltk==3.2.5\n",
    "import nltk\n",
    "nltk.download('perluniprops')\n",
    "nltk.download('nonbreaking_prefixes')\n",
    "\n",
    "import sys, fileinput\n",
    "from nltk.tokenize.moses import MosesTokenizer\n",
    "\n",
    "tokenizer = MosesTokenizer()\n",
    "\n",
    "data = ['i love myself too much.', 'i am so hungry', 'where are you going']\n",
    "\n",
    "for line in data:\n",
    "    if line.strip() != \"\":\n",
    "        tokens = tokenizer.tokenize(line.strip(), escape=False)\n",
    "        tokens[-1] + \"\\n\"  # line 마지막에 \\n 삽입\n",
    "        print(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['where', 'are', 'you', 'going']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
